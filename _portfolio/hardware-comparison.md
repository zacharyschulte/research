---
title: "Hardware Comparison Test: Audio and Video Enhancements"
excerpt: "Establish the value of audio and video integrations by comparing user preference between enhanced and standard experiences."
header:
  teaser: /assets/images/hardware.png
sidebar:
  - title: "Challenge"
    image: http://placehold.it/350x250
    image_alt: "logo"
    text: "Establish the value of client's integrations by comparing user preference between enhanced and standard audio and video experiences."
  - title: "Role"
    text: "Research assistant and test moderation on a large team at End to End User Research."
  - title: "Methods"
    text: "Comparison testing, moderated testing, think-aloud protocol, Likert scale surveys, inferential statistics."
  - title: "Outcomes"
    text: "Client wants further testing to validate integrations on other hardware."
gallery:
  - url: /assets/images/unsplash-gallery-image-1.jpg
    image_path: http://placehold.it/350x250
    alt: "placeholder image 1"
  - url: /assets/images/unsplash-gallery-image-2.jpg
    image_path: http://placehold.it/350x250
    alt: "placeholder image 2"
  - url: /assets/images/unsplash-gallery-image-3.jpg
    image_path: http://placehold.it/350x250
    alt: "placeholder image 3"
---

## Background
The entertainment industry is constantly innovating to offer the best quality of video and audio to their customers. Our client is a leader in this field, with advanced features designed to create a realistic and immersive experience for viewers and listeners. They hired our team to conduct a study to measure how people perceive and value these features compared to the standard options. They also wanted us to suggest how they can promote these features more effectively to potential customers and their laptop vendor partners.

We had less than a month to plan the study, set up the equipment, recruit participants, run the test, and deliver our findings. As a research assistant, my main role was creating the note-taking document used for data analysis, as well as moderating the test sessions.

## Process
First, we established the objectives of the study and designed the methodology. We chose to conduct an in-person moderated test with 36 participants representing a mix of audio and video enthusiasts. We wanted the test to have two distinct parts: one where we assessed user preference between the standard and enhanced audio and video experiences, and the other where we debriefed the participants about our client's integrations and determined their willingness to purchase a laptop with our client's enhancements.

For the first part of the study, we set up four stations with two identical laptops each, representing four different brands. One of each pair included our clients' enhancements, while the other did not. At each station, the participants would either view a video clip simultaneously on each laptop or listen to audio through the laptops' speakers or headphones individually. Since we counterbalanced the order of brand and audio or video experience, we had to make sure to rotate the equipment used at each station. We also created a questionnaire that asked participants to rate the features they experienced in the test, specifically measuring picture clarity, color quality, contrast, sound clarity, immersion, and spatial/directional sound. In order to support the qualitative results from the questionnaire, we collected qualitative feedback from the participants by using the think-aloud protocol and open-ended questions. For the second part of the study, we invited the participants to a second room where we showed them a video about our client's capabilities and asked them about their impressions of the enhanced integrations. Finally, we asked our participants about their willingness to purchase an laptop with our client's enhancements.

My focus for the note-taking document was to ensure that the data collected was comprehensive and comparable. To achieve this, I created a framework that included the variables collected in the questionnaire and the main topics discussed during the debrief. This structure allowed us to easily document any trends or patterns in the responses and to compare the quantitative and qualitative data. After collecting the data from our users, inferential statistics were applied to the qualitative results to examine their significance. We used paired sample t-tests to compare the rating scales between the enhanced and standard experiences for each laptop, and chi-squared tests to compare the overall user preference. This allowed us to compare the mean differences and assess whether the difference was significant.

{% include gallery caption="This is a sample gallery to go along with this case study." %}

Like a good moderator, I always tried to probe participants for more details when they gave vague or general answers. I always used follow-up questions to build on the open-ended questions and dig deeper into their responses, and I found that staying silent was often the best way to encourage participants to say more. However, one challenge I experienced during test moderation was that the participants were not always familiar with terminology in our questionnaire. We asked them to compare aspects of the video experience with terms like "contrast", "picture clarity", "color quality" and aspects of the audio experience with terms like "sound clarity", "immersion", and "spatial/directional sound". To mitigate this, I tried to pay close attention to how the participants described their experience under the think-aloud protocol. I explained these aspects in the participants' own words, and used simple and consistent language to describe them. For example, if a participant talked about how the shadows in the example video seemed too light, I used their example to explain "contrast" as "the difference between the dark and light parts of the picture". Spatial audio was a particiularly difficult concept for participants to grasp, and I relied on examples from the sample video to describe it as "how well the audio conveys the location and distance of the sound sources".

## Insights
The test participants preferred the enhanced audio over the standard experience. To our surprise, though, preferences were split between the enhanced and standard video experience. We found that picture clarity, color quality, and contrast were significantly rated higher for the standard video. We believe this could be due to their unfamiliarity with the enhanced features, as we found that participants who considered themselves as video enthusiasts were more likely to appreciate the enhanced video. We also found that the participants were very interested in the enhanced audio and video features, and that they were willing to pay a premium for a laptop with these features. 

The split in preference was an interesting challenge as we built the report for our client. How did we handle this situation? Well, first of all, we didn't panic. We knew that our data was valid and reliable, and we had to present it honestly and objectively. We also knew that our client was paying us for our expertise and insights, not for telling them what they wanted to hear. So we focused on the positive aspects of the research, and gave some suggestions on how to best strategize based on the user feedback. We began by emphasizing the high preference for the enhanced audio, and how it could be a competitive advantage for our client. We did not ignore or minimize the surprising results. Instead, we described the split preference in detail, and how it could be affected by factors such as the laptop's performance, the sample video clips, and personal taste. We pointed out that user preferences are not always easy to anticipate and that testing is crucial to understand them better. 

Lastly, we stressed how most of the participants were impressed by both the audio and video enhancements after learning more about them in the debriefing room. We advised that our client to provide more education and information about their enhanced video features, as we found that this could increase the appreciation and acceptance of the features. We also recommended that they collaborate with laptop vendors to create bundled offers that highlight our client's enhancements, as this could increase the attractiveness of the features and potentially increase sales. We also offered to conduct further testing to expand on our findings using different video clips and other factors to provide more insight into user preferences. 

## Impact
The client appreciated our honesty and professionalism, and they valued our insights and recommendations. They used the results of our study to guide their promotional strategies. Our study enabled our client to communicate the benefits of their audio/video integrations and how it meets the needs of their partner's target audiences. The client was so satisfied with our work that they asked us to conduct further research on how their integrations are perceived in different contexts, such as vehicle audio.

## Reflection
As a user research expert, I have conducted many studies with different methods and goals. Sometimes, the results are clear and straightforward. Other times, they are surprising and complex. However, user research is not about validating our assumptions or pleasing our clients. It's about finding out what the users really want and need, and being open-minded about what the results mean and what they imply for the study. If you're willing to dig deeper, think creatively, and act flexibly, you can overcome any challenge and deliver great results.

I'd also like to highlight the significance of using language that resonates with participants when eliciting feedback about their experiences. When interacting with users who may not be versed in technical terminology, it is essential to use plain language so as not to mislead them or compromise the validity of the data. Additionally, staying silent can be powerful tool during moderation; often times letting participants talk freely would yield more insightful responses than if I had asked probing follow up questions right away!
