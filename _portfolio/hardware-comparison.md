---
title: "Hardware Comparison Test: Audio and Video Enhancements"
excerpt: "Establish the value of audio and video integrations by comparing user preference between enhanced and standard experiences."
header:
  teaser: /assets/images/hardware.png
sidebar:
  - title: "Challenge"
    image: http://placehold.it/350x250
    image_alt: "logo"
    text: "Establish the value of client's integrations by comparing user preference between enhanced and standard audio and video experiences."
  - title: "Role"
    text: "Research assistant and test moderation on a large team at End to End User Research."
  - title: "Methods"
    text: "Comparison testing, moderated testing, think-aloud protocol, Likert scale surveys, inferential statistics."
  - title: "Outcomes"
    text: "Client wants further testing to validate integrations on other hardware."
gallery:
  - url: /assets/images/unsplash-gallery-image-1.jpg
    image_path: http://placehold.it/350x250
    alt: "placeholder image 1"
  - url: /assets/images/unsplash-gallery-image-2.jpg
    image_path: http://placehold.it/350x250
    alt: "placeholder image 2"
  - url: /assets/images/unsplash-gallery-image-3.jpg
    image_path: http://placehold.it/350x250
    alt: "placeholder image 3"
---

## Background
The entertainment industry is constantly innovating to offer the best quality of video and audio to their customers. Our client is a leader in this field, with advanced features designed to create a realistic and immersive experience for viewers and listeners. They hired our team to conduct a study to measure how people perceive and value these features compared to the standard options. They also wanted us to suggest how they can promote these features more effectively to potential customers and their laptop vendor partners.

We had less than a month to plan the study, set up the equipment, recruit participants, run the test, and deliver our findings. As a research assistant, my main role was creating the note-taking document used for data analysis, as well as moderating the test sessions.

## Process
First, we established the objectives of the study and designed the methodology. We chose to conduct an in-person moderated test with 36 participants representing a mix of audio and video enthusiasts. We wanted the test to have two distinct parts: one where we assessed user preference between the standard and enhanced audio and video experiences, and the other where we debriefed the participants about our client's integrations and determined their willingness to purchase a laptop with our client's enhancements.

For the first part of the study, we set up four stations with two identical laptops each, representing four different brands. One of each pair included our clients' enhancements, while the other did not. At each station, the participants would either view a video clip simultaneously on each laptop or listen to audio through the laptops' speakers or headphones individually. Since we counterbalanced the order of brand and audio or video experience, we had to make sure to rotate the equipment used at each station. We also created a questionnaire that asked participants to rate the features they experienced in the test, specifically measuring picture clarity, color quality, contrast, sound clarity, immersion, and spatial/directional sound. In order to support the qualitative results from the questionnaire, we collected qualitative feedback from the participants by using the think-aloud protocol and open-ended questions. For the second part of the study, we invited the participants to a second room where we showed them a video about our client's capabilities and asked them about their impressions of the enhanced integrations. Finally, we asked our participants about their willingness to purchase an laptop with our client's enhancements.

My focus for the note-taking document was to ensure that the data collected was comprehensive and comparable. To achieve this, I created a framework that included the variables collected in the questionnaire and the main topics discussed during the debrief. This structure allowed us to easily document any trends or patterns in the responses and to compare the quantitative and qualitative data. After collecting the data from our users, inferential statistics were applied to the qualitative results to examine their significance. We used paired sample t-tests to compare the rating scales between the enhanced and standard experiences for each laptop, and chi-squared tests to compare the overall user preference. This allowed us to compare the mean differences and assess whether the difference was significant.

{% include gallery caption="This is a sample gallery to go along with this case study." %}

Like a good moderator, I always tried to probe participants for more details when they gave vague or general answers. I always used follow-up questions to build on the open-ended questions and dig deeper into their responses, and I found that staying silent was often the best way to encourage participants to say more. However, one challenge I experienced during test moderation was that the participants were not always familiar with terminology in our questionnaire. We asked them to compare aspects of the video experience with terms like "contrast", "picture clarity", "color quality" and aspects of the audio experience with terms like "sound clarity", "immersion", and "spatial/directional sound". To mitigate this, I tried to pay close attention to how the participants described their experience under the think-aloud protocol. I explained these aspects in the participants' own words, and used simple and consistent language to describe them. For example, if a participant talked about how the shadows in the example video seemed too light, I used their example to explain "contrast" as "the difference between the dark and light parts of the picture". Spatial audio was a particiularly difficult concept for participants to grasp, and I relied on examples from the sample video to describe it as "how well the audio conveys the location and distance of the sound sources".

## Insights
The test participants preferred the enhanced audio over the standard experience. To our surprise, though, preferences were split between the enhanced and standard video experience. We found that picture clarity, color quality, and contrast were significantly rated higher for the standard video. We believe this could be due to their unfamiliarity with the enhanced features, as we found that participants who considered themselves as video enthusiasts were more likely to appreciate the enhanced video. We also found that the participants were very interested in the enhanced audio and video features, and that they were willing to pay a premium for a laptop with these features. 

The split in preference was an interesting challenge as we built the report for our client. We had to carefully explain why the participants had split preferences, and provide insights as to why they might prefer one or the other. We suggested that they focus on the audio experience in their promotions, as the participants universally preferred the enhanced audio over the standard. We also suggested that they provide more education and information about their enhanced video features, as we concluded that this could lead to increased appreciation and acceptance of the features. Finally, we recommended that they partner with laptop vendors to create bundled packages that include our client's enhanced features, as this could increase the appeal of the features and potentially increase sales.

## Impact
Our client used the findings from our study to inform their promotional strategies. They were able to better understand how their features are perceived by potential customers, and used this information to target the right audiences with their messaging. Our study helped our client understand what customers want from an audio/video experience and how best to promote it, allowing them to refine their strategy for maximum impact. The client requested further research from us to explore how people percieve their integrations in other contexts, such as vehicle audio.

## Reflection
This project was a great opportunity for me to learn about the research process and gain practical experience moderating tests. I learned how to design survey questionnaires, create note-taking documents, apply inferential statistics, and moderate test sessions. In addition, it gave me an appreciation of the complexities involved in designing effective user studies that provide meaningful insights into customer needs and preferences. I discovered that user research results can be surprising and challenging, and that they are not always straightforward. 

One of my biggest takeaways is the importance of using language that is familiar to participants when asking questions about their experiences. When creating surveys or conducting interviews/tests with users who may not be familiar with technical terminology used by professionals in this field, it is key to use clear explanations so as not to confuse them or lead them astray from providing accurate data. Additionally, staying silent can be powerful tool during moderation; often times letting participants talk freely would yield more insightful responses than if I had asked probing follow up questions right away!
